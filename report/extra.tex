\section{Additional Requirements}

In this section, we will discuss any of the additional functionality our impalement has beyond the basic requirements. 

\subsection{Padding}

In the real world, the input file is at an arbitrary size. To handle the file with different length, we use paddings to make sure the file is with the size multiple of the size of data chunks. More specifically, we have added the bits 0 to the end of each file to make sure the file is at the proper length. We have chosen this way because we have assumed that the length of the file is a known value. Therefore, we can read and stop at the end of the files. 

\subsection{Supporting Mutable Files}

File mutation can be categorized as two types. The first case is that one bit of fileâ€™s content has been change but the file length has remain unchanged. Another case it that, some of the data is inserted into/deleted from the original file so that the length of the file is changed. For the first case we can select the changed byte and re-calculate the corresponding $P$ and $Q$ values. For the second case we need to change the file and calculate all the $P$s and $Q$s for every byte following the inserted/deleted byte in the file.
In our implementation, when the file is changed, we compare the changed file with the original data copy store across drives, excluding the $P$ and $Q$ values. Once we find the different data, we replace it using the changed value and re-calculate the $P$ and $Q$ values. It is faster than to replace the whole data file and re-calculate the $P$ and $Q$ from the beginning.

\subsection{larger set of configurations}

One of the RAID 6 basic configurations is that for every 6 data stripes, we generate 2 additional parity blocks to store the additional information in order to tolerate the errors (6+2). However, in our implementation, we do allow the RAID 6 to use an $N_d+2$ ($N_d \geq 2$) configuration. We allow the data to be stored across several data drives, as long as the total number of drives exceeded 3. It enable itself to be used in different configurations where the number of drive can vary. For $N_d = 1$, it is not reasonable to use to additional parity block to tolerate the fault drives. If one would like to do so, he can use RAID 1 twice, which can be faster than RAID 6. The less the number of drives used in the configuration, the higher the percentage of the overhead it generated. Therefore, we recommend to use the configuration $6 + 2$, in which the overhead is relatively low and the overall computation speed is fast.

\subsection{Other RAID implementations}

Beside the RAID 6 implementation, in this project, we have provide functionality to store the data with RAID 5 and RAID 4 format. The data manager can choose to use any of the three configuration to store the data into the drives.

\subsection{Optimization}

There are quite a few optimization opportunities for RAID implementations.

First of all, since we use NumPy's $N$-dimensional array to store the byte type, we make its data type as minimal as possible so that we don't have to wast time computing leading zeros. Due to the $Q$ disk data calculation and checking involve ($\cdot$) in galois field ($\mathbf{GF}(2^8)$), \textsf{numpy.int16} seems to be proper data type that can sufficiently hold the data and support all the operations.

In RAID hardware implementations, the read/write operations are conducted by an intelligent drive controller simultaneously from each component of the independent disk. During our implementation, we also try to make the I/O operations less costly by spawning multiple threads and synchronize them in the main thread.

The largest optimization comes from the parity computations for $P$ and $Q$ disks.

The computation for $P$ is straightforward, for one just needs to calculate the exclusive or of the data. In addition, the CPU usually has the corresponding functionality to do exclusive or operation.

As mentioned in ``The mathematics of RAID-6''\footnote{\url{https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf}}, the biggest problem with RAID-6 is the high CPU cost of computing the $Q$ syndrome. Therefore, to use a fast way to perform multiplication is the key to optimize the operations. We noticed that arbitrary multiplication ($\cdot$) of two elements in galois field is unnecessary since as we limit the generator to be $g=\left\{02\right\}$ we only need to deal with the multiplication with it. This can be improved since the result of \verb|c| multiplication by $\left\{02\right\}$ can be represented by \verb|(c<<1)^((c&0x80)?0x1d:0)|.

In addition, since we use $g=\left\{02\right\}$ frequently, we pre-compute all the distinct values of $g$, that is to say, $GT=\langle g^0, g^1, \ldots, g^{255}\rangle$. The brings us 2 benefits:

\begin{enumerate}
	\item When computing $g^{x}$, we simply look up $GT$ and get its value. The time complexity is $\Theta(1)$.
    \item When computing $log_{g}v$, we only need to check in $GT$ which index has the value $v$ and return the index and the time complexity is $\Theta(n)$. Note that here we can also optimize by constructing a list of a key-value pair $\langle g^{i}, i\rangle$ ($i\in\left\{0,\ldots,255\right\}$) and sort by the key's regular order. In this case, the $log_{g}$ operation can be computed within $\Theta(log{(n)})$.
\end{enumerate}

Taking into vectoring computation into consideration, we also make the choice by heavily relying on vectoring operations (mainly $N$-dimensional arrays) from NumPy package. Although there is no exact evidence that these operations utilize parallelizing instructions provided by the CPU, we believe there must have been some optimizations for parallelism internally.

With all these optimizations, we largely alleviate the performance bottleneck. As a result, for a 3276800-byte data chunk with $N_d=6$, our RAID6 write is finished within $17.52$s and the read operation (with check) is finished within $1.72$s; while with the corresponding RAID-4 write/read operations takes $7.80$s and $0.78$ respectively. We also use an unoptimized implementation of galois field multiplication provided by BitVector-3.3.2 package\footnote{\url{https://engineering.purdue.edu/kak/dist/BitVector-3.3.2.html}} and run it against a 4096-byte data block and the corresponding write/read time is 25.4s/23.8s. This indicates that the optimization is really beneficial.




\section{Lessons Learnt During the Project}
In the project, we have gained the knowledge of how the RAID x achieves the different level of fault tolerance. There are two approaches to ensure the data safety. The first one is to use the data replication (as applied in RAID 1). This approach requires less computation, since one only needs to store the exact same copies of data into different drives. Additionally, the more replication, the more number of disk fault it can tolerant. However, the replication will use much more space, since it duplicates the data. The second approaches is to use error checking code for several  data chucks at a time. This approach will requires more computing resources, since it needs to calculate a proper checking code for the data. However, it is space efficient, for it only generates one or two additional blocks for several blocks of data. The fault tolerance capacity is proportion to the number of checking block it has. 

There are three main criteria for RAID implementations namely, the fault tolerance capacity, the space usage, the computation

One cannot improve one aspect without decrease the other. More specifically, if one wants to tolerant more disk fault, he needs to include more checking/duplicate blocks. There is no silver bullet to choose which RAID should be used to The RAID needs to be configured according to the actual situation

