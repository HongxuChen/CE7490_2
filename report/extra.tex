\section{Additional Requirements}

In this section, we will discuss any of the additional functionality our impalement has beyond the basic requirements. 

\subsection{Padding}

In the real world, the input file is at an arbitrary size. To handle the file with different length, we use paddings to make sure the file is with the size multiple of the size of data chunks. More specifically, we have added the bits 0 to the end of each file 

\subsection{Supporting Mutable Files}

File mutation can be categorized as two types. The first case is that one bit of fileâ€™s content has been change but the file length has remain unchanged. Another case it that, some of the data is inserted into/deleted from the original file so that the length of the file is changed. For the first case we can select the changed byte and re-calculate the corresponding P and Q values. For the second case we need to change the file and calculate all the Ps and Qs for every byte following the inserted/deleted byte in the file.
In our implementation, when the file is changed, we compare the changed file with the original data copy store across drives, excluding the P and Q values. Once we find the different data, we replace it using the changed value and re-calculate the P and Q values. It is faster than to replace the whole data file and re-calculate the P and Q from the beginning.

\subsection{larger set of configurations}

One of the RAID 6 basic configurations is that for every 6 data stripes, we generate 2 additional parity block to store the additional information in order to tolerate the errors (6+2). However, in our implementation, we do allow the RAID 6 to use an $N_d+2$ ($N_d \geq 2$) configuration. We allow the data to be stored across several data drives, as long as the total number of drives exceeded 3. It enable itself to be used in different configurations where the number of drive can vary. For $n = 1$, it is not reasonable to use to additional parity block to tolerate the fault drives. If one would like to do so, he can use RAID 1 twice, which can be faster than RAID 6. The less the number of drives used in the configuration, the higher the percentage of the overhead it generated. Therefore, we recommend to use the configuration $6 + 2$, in which the overhead is relatively low and the overall computation speed is fast.


\subsection{Optimize the computation operations}

The computation for $P$ is straightforward, for one just needs to calculate the exclusive or of the data. In addition, the CPU usually has the corresponding functionality to do exclusive or operation. However, to get $Q$, one needs to invoke Galois field multiplication, which is the most costing part for RAID 6 implementation. Therefore, to use a fast way to perform multiplication is the key to optimize the operations. 

In this project, we have used a pre-computed table for looking up the multiplication values. Since we have choose the $GF(2^8)$ as the field, the possible number of multiplication combinations is limited to $256 \times 256$, which is 65536 cases. We have computed all the 65536 values, which is a one-time cost; and store the value in a table. Each time when we need to do the multiplication, we just need to get the corresponding value from the table with the time complexity $O(1)$. The multiplication is not the bottleneck anymore.

\subsection{Other RAID implementations}

Beside the RAID 6 implementation, in this project, we have provide functionality to store the data with RAID 5 and RAID 4 format. The data manager can choose to use any of the three configuration to store the data into the drives.
