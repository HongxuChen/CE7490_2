\section{System Architecture Overview}

In this section, we will discuss the basic features in RAID 6, the block level striping with double distributed parity architecture.

\subsection{Two Fault Drive Tolerance}
Unlike other RAID architectures, which only allow one drive falls happen at a time, RAID 6 can tolerate two drives break down at the same time. To achieve this, RAID 6 will calculate two parity blocks for a chunk of given data. The first parity block, named $P$ block, is the ``exclusive or'' ($\oplus$) for all the data block. 

\begin{equation}\label{eq:gen_p}
	\mathbf{P = D_0 \oplus D_1 \oplus D_2 \oplus \ldots \oplus D_{n-1}}
\end{equation}

The second parity block, named $Q$ block, requires much more computation. It is defined as the following formula.

\begin{equation}\label{eq:gen_q}
	\mathbf{Q} = g^0\cdot \mathbf{D_0} \oplus g^1\cdot \mathbf{D_1} \oplus g^2\cdot \mathbf{D_2} \oplus \ldots \oplus g^{n-1}\cdot \mathbf{D_{n-1}}
\end{equation}

Here ($\cdot$) is the multiplication operation defined in $\mathbf{GF}(2^8)$ with $\mathrm{m}=100011101$ (corresponding to the irreducible polynomial $x^8+x^4+x^3+x^2+1$) as the modulus. $g$ is the generator and $g^{i}$ is defined as $\underbrace{g\cdot g\ldots g}_{i}$. The addition operation in $\mathbf{GF}(2^8)$ is also XOR ($\oplus$).

With the help of two parity blocks, when two drives fail at the same time, the lost data can be retrieved back from the rest of drives. The detailed retrieval process will be discussed in the implementation section.


\subsection{Balanced Drive Load}
Similar with RAID 5 architecture, RAID 6 also uses the floating parity technique to store the data. It stores the two parity blocks in different drives for different chunks within the data file. Since each of the changes required to rewrite the two parity block, if the parity block is stored only at one drive, the performance of that drive will become the bottleneck of the overall process speed. The updating operation may queue up at that drive, waiting for another update to be finished. RAID 5 and 6 has solved this problem by distributedly deploying the parity block to different drives. Therefore, the load of the drive is balanced, since the update will happen in all the drive instead of the single one.

\subsection{Optimized Reading Performance vs Bad Writing Performance}
RAID 6’s read performance is not affected by adding the two additional parity blocks. Theoretically, RAID 6’s read performance will be as fast as the normal read in the hard disk if only one disk is read at a time. However, in the real world, select different drive to read data due to the floating parity blocks will generate some overhead. Therefore, the real read performance is slightly lower than the RAID 0 architecture with same number of data drives (for n data drives in RAID 6, the comparison is done with n-2 drives for RAID 0).
Despite for the competitive read performance, RAID 6 does impose a write penalty, since each write operation need to read all the data and parity and write all them into the disk. As a result, RAID 6 performs well when the data is read frequently and hardly ever changed once stored in the drives.