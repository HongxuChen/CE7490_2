\subsection{Implementation}

In this section, we will discuss how we implemented the RAID 6 to meet the basic requirement of the project.
We implement our project in Python2.7.9, with the help of NumPy-1.10\footnote{NumPy project homepage: \url{http://www.numpy.org/}} modules.

We implement 3 RAID types: RAID4, RAID5, and RAID6. We use folders in a typical filesystem to simulate these independent disks. We also simplify the data access procedure; that is to say, instead of maintaining the offset of each blocks, we specify a filename for each chunk of data. This is reasonable since the names can be viewed as a starting point of each data set within each disk. The input data are randomly generated with given byte length; and in Unix like systems, this is like generating data from \verb|/dev/urandom|.

The disk number can be configured with parameter $N_{a}$: in RAID4, $N\ge 3$ and $N_{d}=N_{a}-1$ preceding disks are used as data disks with the last one as parity disk; for RAID5, $N_{d}=N_{a}\ge 3$ and the parity byte is distributed among all the disks;  for RAID6, $N_{a}\ge 4$, with $N_{d}=N_{a}-2$ data disks and 1 $P$ disk and 1 $Q$ disk. The value of $N_{d}$ can be extremely large in principle, however for RAID6, since we use $GF^8$ and use byte as the unit, $N_{d}$ is supposed to be $\le 255$.

For all of the implemented RAIDs, we provide \verb|read| and \verb|write| interfaces. The write operation firstly divides the $L$ bytes of data into $N_d$ parts. In case the data length is not all the same, we fill in trailing zeros. 

The integrity check is done internally during each read. The read operation specifies the data starting position (simulated by "file name") and its size. The size is given as a parameter since there is no way to distinguish the trailing from regular data bytes unless storing data block size information, which, however, will make the disk corruption detection and recovery difficult.

For RAID4 and RAID5, we implement data recovery functionality given the corrupted disk index is known. 

For RAID6, we separate the recovery scenarios into the following cases:

\begin{enumerate}
	\item\label{itm:r_d_OR_p} Recover data or $P$ disk (\verb|recover_d_or_p|)
	\item \label{itm:r_q} Recover $Q$ disk (\verb|recover_q|)
	\item \label{itm:r_d_q} Recover both data disk (or $P$ disk) and $Q$ disk (\verb|recover_d_q|)
	\item \label{itm:r_2d} Recover 2 data disks (\verb|recover_2d|)
	\item \label{itm:r_d_p} Recover both data disk and $P$ disk (\verb|recover_d_p|)
\end{enumerate}

It is easy to recovery one disk: for data disk or $P$ disk, we simply calculate the XOR results of other disks excluding $Q$. For $Q$ disk, all we need to do is to recompute it with Equation (\ref{eq:gen_q}).

Similarly, recovering one data disk and $Q$ disk is easy: after computing the XOR parity we get correct data and $P$ disk; then we only need to recompute $Q$ accordingly.

If we lose $\mathbf{D_x}$ \emph{and} $P$ data (Case \ref{itm:r_d_p}), we use the Equation \ref{eq:r_d_p} to retrieve it:

\begin{equation}\label{eq:r_d_p}
	\mathbf{D}_x = (\mathbf{Q}_x+\mathbf{Q})\cdot g^{-x}
\end{equation}

Where $\mathbf{}Q_x$ is the computed $Q$ parity as if $\mathbf{D_x}=\left\{00\right\}$ and for $\mathbf{GF}(2^8)$ we have $g^{-x}=g^{255-x}$. Afterwards, we can get $P$ data.

If we lose 2 data block, namely $\mathbf{D_x}$ and $\mathbf{D_y}$, we use the following results to recover them.

\begin{equation}
	A = g^{y-x}\cdot (g^{y-x}+\left\{01\right\})^{-1}
\end{equation}

\begin{equation}
	B = g^{-x}\cdot(g^{y-x}+\left\{01\right\})^{-1}
\end{equation}

\begin{equation}
	\mathbf{D}_{x} = A\cdot (\mathbf{P}+\mathbf{P}_{xy}) + B\cdot (\mathbf{Q}+\mathbf{Q}_{xy})
\end{equation}

\begin{equation}
	\mathbf{D}_{y} = (\mathbf{P}+\mathbf{P}_{xy}) + \mathbf{D}_x
\end{equation}

Here $\mathbf{P}_{xy}$ and $\mathbf{Q}_{xy}$ are computed parity as if $\mathbf{D}_x=\left\{00\right\}$ and $\mathbf{D}_y=\left\{00\right\}$.





